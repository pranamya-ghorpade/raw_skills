{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "import gensim.corpora as corpora\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import string\n",
    "import scipy\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import wordnet as wn,stopwords\n",
    "from nltk import CFG,word_tokenize\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,WordNetLemmatizer\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from textblob import TextBlob,Word\n",
    "from spacy import displacy\n",
    "from bs4 import BeautifulSoup\n",
    "from afinn import Afinn\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Bidirectional,Embedding,SpatialDropout1D,Flatten,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers ,regularizers\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=pd.read_csv('Raw_Skills_Dataset.csv')\n",
    "tech=pd.read_csv('Example_Technical_Skills.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAW DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What ifs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seniority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>familiarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>functionalities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambdas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RAW DATA\n",
       "0         What ifs\n",
       "1        seniority\n",
       "2      familiarity\n",
       "3  functionalities\n",
       "4          Lambdas"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAP Fiori Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oracle Instance Management &amp; Strategy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boomi Master Data Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Manufacturing on Cloud ( DMC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DevOps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Technology Skills\n",
       "0                    SAP Fiori Developer\n",
       "1  Oracle Instance Management & Strategy\n",
       "2           Boomi Master Data Management\n",
       "3  Digital Manufacturing on Cloud ( DMC)\n",
       "4                                 DevOps"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34116, 1) (979, 1)\n"
     ]
    }
   ],
   "source": [
    "print(raw.shape,tech.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach Used:\n",
    "\n",
    "1. Gather the sentence vector form for technical data\n",
    "2. Gather the sentence vector form for raw data\n",
    "3. For every skill vector in raw data find the cosine similarity btw it and the tech data and if its above the threshold mark it hard skill "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach with sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "     ---------------------------------------- 79.7/79.7 KB 4.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 33.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\prana\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.50.2)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "     ------------------------------------- 158.0/158.0 MB 10.9 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 32.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\prana\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\prana\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\prana\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\prana\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 17.2 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 KB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\prana\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: requests in c:\\users\\prana\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.10.15)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 26.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.4)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: filelock in c:\\users\\prana\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 40.8/40.8 KB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\prana\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\prana\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\prana\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\prana\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120753 sha256=9161059e3a8217d4cb2aca03860948c5913127e158780b83ead3c1ae0bd91504\n",
      "  Stored in directory: c:\\users\\prana\\appdata\\local\\pip\\cache\\wheels\\0c\\b6\\fb\\2289a932c365293ad865fc1fe9d2db694d5584241c6d670874\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, packaging, torchvision, huggingface-hub, transformers, sentence-transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed huggingface-hub-0.7.0 packaging-21.3 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1e64d0f095448ba0ef3a40108d062b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246bbe2d571c4fe99ad63e87800da920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=190.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667286f928a49d98f579cf2e13d9ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=3988.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3037a4e2e1f459d91485441f8fff0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=550.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b838f0af4f471d85a415c1eac341f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a90b40113034999a88525f5d88a5e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b3793d96104f19ae3b312aefd832eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=265486777.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2e3587ecf4884a45888f5b47c53a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=53.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5322bcd30a24535b5ddf43ea69e2447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5682e61d16548af88ede0027d639abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb76fea1e0b429597e0e8a51f1330ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbb04faa3934f43b457f30c5908734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "raw_skills=raw['RAW DATA']\n",
    "tech_skills=tech['Technology Skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_embeddings=model.encode(raw_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_embeddings=model.encode(tech_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list=[]\n",
    "for i in raw_embeddings:\n",
    "    listed=[]\n",
    "    for j in tech_embeddings[0:1500]:\n",
    "        ans=1 - distance.cosine(i,j)\n",
    "        if ans>0.5:\n",
    "            listed.append('yes')\n",
    "        else:\n",
    "            listed.append('no')\n",
    "    check_list.append(max(set(listed),key=listed.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_list[0:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['is_it_hard_skill']=pd.Series(check_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAW DATA</th>\n",
       "      <th>is_it_hard_skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What ifs</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seniority</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>familiarity</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>functionalities</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambdas</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RAW DATA is_it_hard_skill\n",
       "0         What ifs               no\n",
       "1        seniority               no\n",
       "2      familiarity               no\n",
       "3  functionalities              yes\n",
       "4          Lambdas              yes"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    20892\n",
       "no     13224\n",
       "Name: is_it_hard_skill, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['is_it_hard_skill'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the hard skills detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   RAW DATA is_it_hard_skill\n",
      "693                     the project roadmap              yes\n",
      "694                   emerging technologies              yes\n",
      "695                              frameworks              yes\n",
      "696                  cloud-based ecosystems              yes\n",
      "698                                 scripts              yes\n",
      "700                 production environments              yes\n",
      "702                                     Vue              yes\n",
      "703                                 .. APIs              yes\n",
      "704                     emerging frameworks              yes\n",
      "706                                    Node              yes\n",
      "707                              Typescript              yes\n",
      "711                                     GCP              yes\n",
      "712                Operational efficiencies              yes\n",
      "721                                     API              yes\n",
      "722                           UI components              yes\n",
      "723                               the norms              yes\n",
      "724                   emerging technologies              yes\n",
      "725                              frameworks              yes\n",
      "726                  cloud-based ecosystems              yes\n",
      "730                                 scripts              yes\n",
      "732                 production environments              yes\n",
      "734                                     Vue              yes\n",
      "735                                 .. APIs              yes\n",
      "736                     emerging frameworks              yes\n",
      "738                                    Node              yes\n",
      "739                              Typescript              yes\n",
      "743                                     GCP              yes\n",
      "744                Operational efficiencies              yes\n",
      "749                   a velocity multiplier              yes\n",
      "752                                     API              yes\n",
      "753                           UI components              yes\n",
      "754                               the norms              yes\n",
      "757                               oversight              yes\n",
      "758                     the project roadmap              yes\n",
      "759                   emerging technologies              yes\n",
      "760                              frameworks              yes\n",
      "761                  cloud-based ecosystems              yes\n",
      "763                                 scripts              yes\n",
      "765                 production environments              yes\n",
      "766                      Automated analysis              yes\n",
      "767                                      UI              yes\n",
      "768                         Data annotation              yes\n",
      "770                         a biotechnology              yes\n",
      "772                         cloud computing              yes\n",
      "773                      automated analysis              yes\n",
      "774                              annotation              yes\n",
      "775  advanced display and data manipulation              yes\n",
      "779                           documentation              yes\n",
      "780                            your sleeves              yes\n",
      "784                    genomic file formats              yes\n"
     ]
    }
   ],
   "source": [
    "print(raw.loc[raw['is_it_hard_skill']=='yes'][450:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
